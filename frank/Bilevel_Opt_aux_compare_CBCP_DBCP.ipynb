{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axe\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import cvxpy as cp\n",
    "\n",
    "import copy\n",
    "\n",
    "import random\n",
    "from itertools import chain, combinations, tee\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd5df",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demand_name_by_group_index(index):\n",
    "    list_demand_names = [\"Demand (eligible group, 1)\", \"Demand (eligible group, 2)\", \\\n",
    "                         \"Demand (ineligible group, 1)\", \"Demand (ineligible group, 2)\", \\\n",
    "                         \"Demand (ineligible group, 3)\"]\n",
    "    return list_demand_names[index]\n",
    "\n",
    "def VoT_name_by_group_index(index):\n",
    "    list_demand_names = [\"VoT (eligible group, 1)\", \"VoT (eligible group, 2)\", \\\n",
    "                         \"VoT (ineligible group, 1)\", \"VoT (ineligible group, 2)\", \\\n",
    "                         \"VoT (ineligible group, 3)\"]\n",
    "    return list_demand_names[index]\n",
    "\n",
    "def equals(a, b, tol = 1E-3):\n",
    "    if abs(a-b) <= tol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def equals_array(arr_1, arr_2, tol = 1E-2):\n",
    "    if np.linalg.norm(arr_1 - arr_2) <= tol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_max(flow_max, coeff):\n",
    "    \n",
    "    assert np.all(coeff >= 0.0), \"coeff should be non-negative\"\n",
    "    assert len(coeff.shape) == 1, \"coeff should be a 1-D array.\"\n",
    "    assert coeff.shape[0] == 3, \"Latency functions are assumed to be piecewise linear / affine with 3 parameters.\"\n",
    "    \n",
    "    return coeff[0] + max(coeff[1] * (flow_max - coeff[2]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_sig_figs(number, sig_figs):\n",
    "    if number == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate the exponent for scientific notation\n",
    "    exponent = math.floor(math.log10(abs(number)))\n",
    "    \n",
    "    # Calculate the scaling factor to bring the number to a single digit before the decimal\n",
    "    scale = 10**(sig_figs - 1 - exponent)\n",
    "    \n",
    "    # Scale, round, and then unscale\n",
    "    rounded_number = round(number * scale) / scale\n",
    "    \n",
    "    return rounded_number\n",
    "\n",
    "def round_to_sig_figs_array_1D(arr, sig_figs):\n",
    "    \n",
    "    arr_rounded = np.zeros(arr.shape)\n",
    "    \n",
    "    for i in range(arr.shape[0]):\n",
    "        arr_rounded[i] = round_to_sig_figs(arr[i], sig_figs)\n",
    "            \n",
    "    return arr_rounded\n",
    "\n",
    "def round_to_sig_figs_array_2D(arr, sig_figs):\n",
    "    \n",
    "    arr_rounded = np.zeros(arr.shape)\n",
    "    \n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            arr_rounded[i, j] = round_to_sig_figs(arr[i, j], sig_figs)\n",
    "            \n",
    "    return arr_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f582a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "# np.linalg.norm(arr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef57c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f610a20e",
   "metadata": {},
   "source": [
    "# Download Groups, Routes to Edges Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcefd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '../data/data_income_percentage_VoT___101_N_Sep_to_Nov_2024/'\n",
    "# df_data = pd.read_csv(directory_path + 'data_cities_od_VoTs_demands_1.csv')\n",
    "df_data = pd.read_csv(directory_path + 'data_cities_od_VoTs_demands_3.csv')\n",
    "\n",
    "# df_od_flow_data\n",
    "# df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {}\n",
    "\n",
    "for column_name_full in list(df_data.columns):\n",
    "    if column_name_full == \"Data Category\":\n",
    "        categories_list = df_data[column_name_full].tolist()\n",
    "    else:\n",
    "        dict_data[int(column_name_full)] = {}\n",
    "        for category_index, category in enumerate(categories_list):\n",
    "            if category == \"Start City Index\" or category == \"End City Index\":\n",
    "                dict_data[int(column_name_full)][category] \\\n",
    "                    = int(df_data[column_name_full].tolist()[category_index])\n",
    "            elif category == \"Start City\" or category == \"End City\":\n",
    "                dict_data[int(column_name_full)][category] \\\n",
    "                    = df_data[column_name_full].tolist()[category_index]\n",
    "            else:\n",
    "#                 print(\"category:\", category)\n",
    "                dict_data[int(column_name_full)][category] \\\n",
    "                    = float(df_data[column_name_full].tolist()[category_index])\n",
    "\n",
    "# Test git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da0a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dict = {}\n",
    "for od_info in list(dict_data.values()):\n",
    "    if od_info[\"Start City Index\"] not in list(cities_dict.keys()):\n",
    "        cities_dict[od_info[\"Start City Index\"]] = od_info[\"Start City\"]\n",
    "    if od_info[\"End City Index\"] not in list(cities_dict.keys()):\n",
    "        cities_dict[od_info[\"End City Index\"]] = od_info[\"End City\"]\n",
    "\n",
    "cities_list = list(cities_dict.values())\n",
    "\n",
    "# cities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_to_edges_array = np.zeros((len(list(dict_data.keys())), 2))\n",
    "\n",
    "for od_index, od_info in dict_data.items():\n",
    "    od_to_edges_array[od_index, 0] = int(cities_list.index(od_info[\"Start City\"]))\n",
    "    od_to_edges_array[od_index, 1] = int(cities_list.index(od_info[\"End City\"]))\n",
    "\n",
    "edge_to_od_dict = {}\n",
    "num_edges = int(np.max(od_to_edges_array)) + 1\n",
    "# print(\"num_edges:\", num_edges)\n",
    "\n",
    "for e in range(num_edges):\n",
    "    edge_to_od_dict[e] = [k for k in list(range(int(od_to_edges_array.shape[0]) )) \\\n",
    "                           if od_to_edges_array[k, 0] <= e <= od_to_edges_array[k, 1]]\n",
    "    \n",
    "# od_to_edges_array\n",
    "# edge_to_od_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_groups_per_od = 5\n",
    "\n",
    "demand_array = np.zeros((len(list(dict_data.keys())), num_groups_per_od))\n",
    "VoT_array_base = np.zeros((len(list(dict_data.keys())), num_groups_per_od))\n",
    "\n",
    "for od_index, od_value in dict_data.items():\n",
    "    for group_index in range(num_groups_per_od):\n",
    "        demand_name = demand_name_by_group_index(group_index)\n",
    "        VoT_name = VoT_name_by_group_index(group_index)\n",
    "        \n",
    "        demand_array[od_index, group_index] = od_value[demand_name]\n",
    "        VoT_array_base[od_index, group_index] = od_value[VoT_name]\n",
    "\n",
    "print(demand_array)\n",
    "# VoT_array_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '../data/data_income_percentage_VoT___101_N_Sep_to_Nov_2024/'\n",
    "\n",
    "T = 5\n",
    "VoT_array = np.zeros((VoT_array_base.shape[0], VoT_array_base.shape[1], T))\n",
    "\n",
    "for t in range(T):\n",
    "    df_perturbation_data = pd.read_csv(directory_path + 'perturbations_1_' + str(t) + '.csv')\n",
    "    perturbation_array = df_perturbation_data.to_numpy()[:, 1:]\n",
    "    VoT_array[:, :, t] = VoT_array_base * perturbation_array\n",
    "    \n",
    "# VoT_array_base\n",
    "# perturbation_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ea18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "622716c5",
   "metadata": {},
   "source": [
    "# Download Latency Parameters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path_latency = '../data/pems_latency_inference___101_N_Sep_to_Nov_2024/'\n",
    "df_latency_params = pd.read_csv(directory_path_latency + 'latency_params.csv')\n",
    "\n",
    "# list(df_latency_params.loc[:, \"Palo Alto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c09a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_latency_params = {}\n",
    "\n",
    "city_list = list(df_latency_params.columns)[1:]\n",
    "\n",
    "for city in city_list:\n",
    "#     if city != \"Belmont\":\n",
    "    if 1 == 1:\n",
    "        dict_latency_params[city] = {}\n",
    "        dict_latency_params[city][\"Flow (at bend)\"] = df_latency_params.loc[:, city][0]\n",
    "        dict_latency_params[city][\"Latency (at bend)\"] = df_latency_params.loc[:, city][1]\n",
    "        dict_latency_params[city][\"Slope (after bend)\"] = df_latency_params.loc[:, city][2]\n",
    "\n",
    "dict_latency_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7e1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_edges = 7\n",
    "num_gp_lanes = 3\n",
    "\n",
    "num_el = 2\n",
    "num_groups = demand_array.shape[1]\n",
    "\n",
    "el_indices = list(range(num_el))\n",
    "in_indices = list(range(num_el, num_groups))\n",
    "\n",
    "coeff_input = np.zeros((3, num_edges))\n",
    "for counter, city in enumerate(dict_latency_params.keys()):\n",
    "    coeff_input[0, counter] = dict_latency_params[city][\"Latency (at bend)\"]\n",
    "    coeff_input[1, counter] = dict_latency_params[city][\"Slope (after bend)\"]\n",
    "    coeff_input[2, counter] = dict_latency_params[city][\"Flow (at bend)\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set lambdas:\n",
    "\n",
    "lambda_E, lambda_R, lambda_I = 1.0, 1.0, 1.0\n",
    "\n",
    "## Initialize tau, alpha values:\n",
    "\n",
    "filename_segment = str(int(lambda_E)) + '_' + str(int(lambda_R)) + '_' + str(int(lambda_I))\n",
    "\n",
    "directory_inits = '../data/opt_CBCP_values___' + str(num_el) + '_el_groups/'\n",
    "# directory_inits = '../data/opt_CBCP_values___' + str(num_el) + '_el_groups___before_20251001/'\n",
    "df_inits = pd.read_csv(directory_inits + filename_segment + '___tau_B_stats_CBCP.csv')\n",
    "\n",
    "print(\"filename_segment:\", filename_segment)\n",
    "print()\n",
    "\n",
    "inits_tau_arr_as_object = df_inits.to_numpy()[:, 1:6]\n",
    "inits_B_arr_as_object = df_inits.to_numpy()[0, 7]\n",
    "\n",
    "argmin_tau = np.zeros((num_edges, T))\n",
    "argmin_B = 0\n",
    "\n",
    "for e in range(num_edges):\n",
    "    for t in range(T):\n",
    "        argmin_tau[e, t] = inits_tau_arr_as_object[e, t]\n",
    "        argmin_B = inits_B_arr_as_object\n",
    "\n",
    "print(\"argmin_tau:\\n\", argmin_tau)\n",
    "print()\n",
    "print(\"argmin_B:\\n\", argmin_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmin_tau = np.array([[0.0, 0.3194, 0.3194, 0.0, 0.0], \\\n",
    "#                        [0.2498, 0.2498, 0.2498, 0.0, 0.0], \\\n",
    "#                        [0.0, 0.0, 0.0, 0.9995, 0.9995], \\\n",
    "#                        [0.0, 1.0281, 1.0281, 1.0281, 0.0], \\\n",
    "#                        [1.6043, 0.0, 0.0, 1.6043, 0.0], \\\n",
    "#                        [0.0, 0.1922, 0.1922, 0.1922, 0.0], \\\n",
    "#                        [0.0, 0.0, 0.0, 0.2178, 0.2178]])\n",
    "\n",
    "# argmin_B = 10.6925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c35f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51ee44e",
   "metadata": {},
   "source": [
    "# Compute CBCP and DBCP comparison statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand_array\n",
    "# edge_to_od_dict\n",
    "# od_to_edges_array\n",
    "\n",
    "# demand_array[:, 0:3]\n",
    "# demand_array[:, 3:5]\n",
    "# demand_array\n",
    "\n",
    "# np.sum(demand_array[:, 0:3], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_array_el = np.sum(demand_array[:, 0:3], axis=1)\n",
    "demand_array_in = np.sum(demand_array[:, 3:5], axis=1)\n",
    "\n",
    "demand_array_el_across_edges = np.zeros(num_edges)\n",
    "demand_array_in_across_edges = np.zeros(num_edges)\n",
    "demand_array_across_edges = np.zeros(num_edges)\n",
    "\n",
    "for e in range(num_edges):\n",
    "    demand_array_el_across_edges[e] = np.sum([demand_array_el[od] for od in edge_to_od_dict[e]])\n",
    "    demand_array_in_across_edges[e] = np.sum([demand_array_in[od] for od in edge_to_od_dict[e]])\n",
    "    demand_array_across_edges[e] = demand_array_el_across_edges[e] + demand_array_in_across_edges[e]\n",
    "\n",
    "# demand_array_el_across_edges[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify that demand_array_across_edges = demand_array_el_across_edges + demand_array_in_across_edges\n",
    "\n",
    "print(\"demand_array_el_across_edges:\", demand_array_el_across_edges)\n",
    "print(\"demand_array_in_across_edges:\", demand_array_in_across_edges)\n",
    "print(\"demand_array_across_edges:\", demand_array_across_edges)\n",
    "print()\n",
    "\n",
    "demand_array_consistent_tolerance = 1E-2\n",
    "demand_array_consistent = np.all(np.abs(demand_array_across_edges \\\n",
    "                                        - demand_array_el_across_edges - demand_array_in_across_edges) \\\n",
    "                                <= demand_array_consistent_tolerance)\n",
    "\n",
    "print(demand_array_consistent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bb780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb3450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv and store into numpy files\n",
    "\n",
    "directory_CBCP = '../data/opt_CBCP_values___' + str(num_el) + '_el_groups/'\n",
    "directory_DBCP = '../data/opt_DBCP_values___' + str(num_el) + '_el_groups/'\n",
    "# directory_CBCP = '../data/opt_CBCP_values___' + str(num_el) + '_el_groups___before_20251001/'\n",
    "# directory_DBCP = '../data/opt_DBCP_values___' + str(num_el) + '_el_groups___before_20251001/'\n",
    "\n",
    "tau_CBCP_dict = {}\n",
    "tau_time_averaged_CBCP_dict = {}\n",
    "B_CBCP_dict = {}\n",
    "percent_express_lane_use_CBCP_dict = {}\n",
    "avg_travel_time_CBCP_dict = {}\n",
    "total_costs_CBCP_dict = {}\n",
    "\n",
    "tau_DBCP_dict = {}\n",
    "tau_time_averaged_DBCP_dict = {}\n",
    "alpha_DBCP_dict = {}\n",
    "percent_express_lane_use_DBCP_dict = {}\n",
    "avg_travel_time_DBCP_dict = {}\n",
    "total_costs_DBCP_dict = {}\n",
    "\n",
    "\n",
    "# lambdas_array = np.array([[1.0, 1.0, 1.0], \\\n",
    "#                           [1.0, 5.0, 1.0], \\\n",
    "#                           [1.0, 10.0, 1.0], \\\n",
    "#                           [5.0, 5.0, 1.0], \\\n",
    "#                           [5.0, 10.0, 1.0], \\\n",
    "#                           [10.0, 10.0, 1.0], \\\n",
    "#                           [1.0, 1.0, 0.0], \\\n",
    "#                           [1.0, 5.0, 0.0], \\\n",
    "#                           [5.0, 10.0, 0.0]])\n",
    "\n",
    "lambdas_array = np.array([[1.0, 1.0, 1.0], \\\n",
    "                          [1.0, 5.0, 1.0], \\\n",
    "                          [1.0, 10.0, 1.0], \\\n",
    "                          [5.0, 5.0, 1.0], \\\n",
    "                          [5.0, 10.0, 1.0], \\\n",
    "                          [10.0, 10.0, 1.0], \\\n",
    "                          [1.0, 1.0, 0.0], \\\n",
    "                          [1.0, 5.0, 0.0], \\\n",
    "                          [5.0, 10.0, 0.0], \\\n",
    "                          [5.0, 1.0, 1.0], \\\n",
    "                          [10.0, 5.0, 1.0], \\\n",
    "                          [10.0, 1.0, 1.0], \\\n",
    "                          [20.0, 1.0, 1.0], \\\n",
    "                          [5.0, 1.0, 0.0], \\\n",
    "                          [10.0, 5.0, 0.0], \\\n",
    "                          [10.0, 1.0, 0.0], \\\n",
    "                          [20.0, 1.0, 0.0], \\\n",
    "                          [5.0, 0.0, 1.0], \\\n",
    "                          [10.0, 0.0, 1.0], \\\n",
    "                          [20.0, 0.0, 1.0], \\\n",
    "                         ])\n",
    "\n",
    "for lambdas_index in range(lambdas_array.shape[0]):\n",
    "    lambda_E, lambda_R, lambda_I = lambdas_array[lambdas_index]\n",
    "    \n",
    "    filename_segment = str(int(lambda_E)) + '_' + str(int(lambda_R)) + '_' + str(int(lambda_I))\n",
    "    \n",
    "    lambda_E, lambda_R, lambda_I = lambdas_array[lambdas_index]\n",
    "    lambdas = (lambda_E, lambda_R, lambda_I)\n",
    "    filename_segment = str(int(lambda_E)) + '_' + str(int(lambda_R)) + '_' + str(int(lambda_I))\n",
    "\n",
    "    # Inputting CBCP data:\n",
    "\n",
    "    df_CBCP = pd.read_csv(directory_CBCP + filename_segment + '___tau_B_stats_CBCP.csv')\n",
    "    tau_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 1:6].astype(float)\n",
    "    tau_time_averaged_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 6].astype(float)\n",
    "    B_CBCP_dict[lambdas] = df_CBCP.to_numpy()[0, 7]\n",
    "    percent_express_lane_use_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 8:11].astype(float)\n",
    "    avg_travel_time_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 11:13].astype(float)\n",
    "    total_costs_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 13:].astype(float)\n",
    "\n",
    "    # Inputting DBCP data:\n",
    "\n",
    "    df_DBCP = pd.read_csv(directory_DBCP + filename_segment + '___tau_alpha_stats_DBCP.csv')\n",
    "    tau_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 1:6].astype(float)\n",
    "    tau_time_averaged_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 6].astype(float)\n",
    "    alpha_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 7:12].astype(float)\n",
    "    percent_express_lane_use_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 12:15].astype(float)\n",
    "    avg_travel_time_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 15:17].astype(float)\n",
    "    total_costs_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 17:].astype(float)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  total_costs_DBCP_dict[(5.0, 10.0, 1.0)]\n",
    "\n",
    "df_DBCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47137d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data into average or total costs, and storing into dict_avg_stats:\n",
    "\n",
    "dict_avg_stats = {}\n",
    "\n",
    "for lambdas_index in range(lambdas_array.shape[0]):\n",
    "    lambda_E, lambda_R, lambda_I = lambdas_array[lambdas_index]\n",
    "    lambdas = (lambda_E, lambda_R, lambda_I)\n",
    "\n",
    "    # Processing CBCP data:\n",
    "    \n",
    "    dict_index = (lambda_E, lambda_R, lambda_I, 'CBCP')\n",
    "    \n",
    "    dict_avg_stats[dict_index] = {}\n",
    "    \n",
    "    dict_avg_stats[dict_index]['percent of overall'] \\\n",
    "        = np.sum(percent_express_lane_use_CBCP_dict[lambdas][:, 0] * demand_array_across_edges) \\\n",
    "            / np.sum(demand_array_across_edges)\n",
    "    dict_avg_stats[dict_index]['percent of eligible'] \\\n",
    "        = np.sum(percent_express_lane_use_CBCP_dict[lambdas][:, 1] * demand_array_el_across_edges) \\\n",
    "            / np.sum(demand_array_el_across_edges)\n",
    "    dict_avg_stats[dict_index]['percent of ineligible'] \\\n",
    "        = np.sum(percent_express_lane_use_CBCP_dict[lambdas][:, 2] * demand_array_in_across_edges) \\\n",
    "            / np.sum(demand_array_in_across_edges)\n",
    "    \n",
    "    dict_avg_stats[dict_index]['average travel time, express'] \\\n",
    "        = np.sum(avg_travel_time_CBCP_dict[lambdas][:, 0])\n",
    "    dict_avg_stats[dict_index]['average travel time, general purpose'] \\\n",
    "        = np.sum(avg_travel_time_CBCP_dict[lambdas][:, 1])\n",
    "    \n",
    "    dict_avg_stats[dict_index]['total travel cost, eligible'] \\\n",
    "        = np.sum(total_costs_CBCP_dict[lambdas][:, 0])\n",
    "    dict_avg_stats[dict_index]['total travel cost, ineligible'] \\\n",
    "        = np.sum(total_costs_CBCP_dict[lambdas][:, 1])\n",
    "    dict_avg_stats[dict_index]['total toll revenue'] \\\n",
    "        = np.sum(total_costs_CBCP_dict[lambdas][:, 2])\n",
    "    dict_avg_stats[dict_index]['total societal cost'] \\\n",
    "        = np.sum(total_costs_CBCP_dict[lambdas][:, 3])\n",
    "\n",
    "#     # Processing DBCP data:\n",
    "    \n",
    "    dict_index = (lambda_E, lambda_R, lambda_I, 'DBCP')\n",
    "    \n",
    "    dict_avg_stats[dict_index] = {}\n",
    "    \n",
    "    dict_avg_stats[dict_index]['percent of overall'] \\\n",
    "        = np.sum(percent_express_lane_use_DBCP_dict[lambdas][:, 0] * demand_array_across_edges) \\\n",
    "            / np.sum(demand_array_across_edges)\n",
    "    dict_avg_stats[dict_index]['percent of eligible'] \\\n",
    "        = np.sum(percent_express_lane_use_DBCP_dict[lambdas][:, 1] * demand_array_el_across_edges) \\\n",
    "            / np.sum(demand_array_el_across_edges)\n",
    "    dict_avg_stats[dict_index]['percent of ineligible'] \\\n",
    "        = np.sum(percent_express_lane_use_DBCP_dict[lambdas][:, 2] * demand_array_in_across_edges) \\\n",
    "            / np.sum(demand_array_in_across_edges)\n",
    "    \n",
    "    dict_avg_stats[dict_index]['average travel time, express'] \\\n",
    "        = np.sum(avg_travel_time_DBCP_dict[lambdas][:, 0])\n",
    "    dict_avg_stats[dict_index]['average travel time, general purpose'] \\\n",
    "        = np.sum(avg_travel_time_DBCP_dict[lambdas][:, 1])\n",
    "    \n",
    "    dict_avg_stats[dict_index]['total travel cost, eligible'] \\\n",
    "        = np.sum(total_costs_DBCP_dict[lambdas][:, 0])\n",
    "    dict_avg_stats[dict_index]['total travel cost, ineligible'] \\\n",
    "        = np.sum(total_costs_DBCP_dict[lambdas][:, 1])\n",
    "    dict_avg_stats[dict_index]['total toll revenue'] \\\n",
    "        = np.sum(total_costs_DBCP_dict[lambdas][:, 2])\n",
    "    dict_avg_stats[dict_index]['total societal cost'] \\\n",
    "        = np.sum(total_costs_DBCP_dict[lambdas][:, 3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce192c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambdas = (1.0, 1.0, 1.0)\n",
    "\n",
    "# percent_express_lane_use_CBCP_dict[lambdas][:, 0]\n",
    "\n",
    "# np.sum(percent_express_lane_use_CBCP_dict[lambdas][:, 0] * demand_array_across_edges) \\\n",
    "#             / np.sum(demand_array_across_edges)\n",
    "\n",
    "# dict_avg_stats[(1.0, 1.0, 1.0, 'CBCP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754103d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stats_array = np.zeros((lambdas_array.shape[0] * 2, 13))\n",
    "\n",
    "for lambdas_index in range(lambdas_array.shape[0]):\n",
    "    lambda_E, lambda_R, lambda_I = lambdas_array[lambdas_index]\n",
    "    \n",
    "    # Storing CBCP avg stats as array:\n",
    "    \n",
    "    dict_index = (lambda_E, lambda_R, lambda_I, 'CBCP')\n",
    "    \n",
    "    avg_stats_array[2 * lambdas_index, 0] = lambda_E\n",
    "    avg_stats_array[2 * lambdas_index, 1] = lambda_R\n",
    "    avg_stats_array[2 * lambdas_index, 2] = lambda_I\n",
    "    \n",
    "    # Here, \"0\" indicates CBCP\n",
    "    avg_stats_array[2 * lambdas_index, 3] = 0\n",
    "    \n",
    "    avg_stats_array[2 * lambdas_index, 4] = dict_avg_stats[dict_index]['percent of overall']\n",
    "    avg_stats_array[2 * lambdas_index, 5] = dict_avg_stats[dict_index]['percent of eligible']\n",
    "    avg_stats_array[2 * lambdas_index, 6] = dict_avg_stats[dict_index]['percent of ineligible']\n",
    "    \n",
    "    avg_stats_array[2 * lambdas_index, 7] = dict_avg_stats[dict_index]['average travel time, express']\n",
    "    avg_stats_array[2 * lambdas_index, 8] = dict_avg_stats[dict_index]['average travel time, general purpose']\n",
    "\n",
    "    avg_stats_array[2 * lambdas_index, 9] = dict_avg_stats[dict_index]['total travel cost, eligible']\n",
    "    avg_stats_array[2 * lambdas_index, 10] = dict_avg_stats[dict_index]['total travel cost, ineligible']\n",
    "    avg_stats_array[2 * lambdas_index, 11] = dict_avg_stats[dict_index]['total toll revenue']\n",
    "    avg_stats_array[2 * lambdas_index, 12] = dict_avg_stats[dict_index]['total societal cost']\n",
    "    \n",
    "    # Storing DBCP avg stats as array:\n",
    "    \n",
    "    dict_index = (lambda_E, lambda_R, lambda_I, 'DBCP')\n",
    "    \n",
    "    avg_stats_array[2 * lambdas_index + 1, 0] = lambda_E\n",
    "    avg_stats_array[2 * lambdas_index + 1, 1] = lambda_R\n",
    "    avg_stats_array[2 * lambdas_index + 1, 2] = lambda_I\n",
    "    \n",
    "    # Here, \"1\" indicates DBCP\n",
    "    avg_stats_array[2 * lambdas_index + 1, 3] = 1\n",
    "    \n",
    "    avg_stats_array[2 * lambdas_index + 1, 4] = dict_avg_stats[dict_index]['percent of overall']\n",
    "    avg_stats_array[2 * lambdas_index + 1, 5] = dict_avg_stats[dict_index]['percent of eligible']\n",
    "    avg_stats_array[2 * lambdas_index + 1, 6] = dict_avg_stats[dict_index]['percent of ineligible']\n",
    "    \n",
    "    avg_stats_array[2 * lambdas_index + 1, 7] = dict_avg_stats[dict_index]['average travel time, express']\n",
    "    avg_stats_array[2 * lambdas_index + 1, 8] = dict_avg_stats[dict_index]['average travel time, general purpose']\n",
    "\n",
    "    avg_stats_array[2 * lambdas_index + 1, 9] = dict_avg_stats[dict_index]['total travel cost, eligible']\n",
    "    avg_stats_array[2 * lambdas_index + 1, 10] = dict_avg_stats[dict_index]['total travel cost, ineligible']\n",
    "    avg_stats_array[2 * lambdas_index + 1, 11] = dict_avg_stats[dict_index]['total toll revenue']\n",
    "    avg_stats_array[2 * lambdas_index + 1, 12] = dict_avg_stats[dict_index]['total societal cost']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stats_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a57ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stats_array_rounded = np.zeros(avg_stats_array.shape)\n",
    "\n",
    "for lambdas_index in range(lambdas_array.shape[0]):\n",
    "    lambda_E, lambda_R, lambda_I = lambdas_array[lambdas_index]\n",
    "    \n",
    "    # Storing CBCP avg stats as array:\n",
    "    \n",
    "    dict_index = (lambda_E, lambda_R, lambda_I, 'CBCP')\n",
    "    \n",
    "    avg_stats_array_rounded[2 * lambdas_index, 0] = lambda_E\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 1] = lambda_R\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 2] = lambda_I\n",
    "    \n",
    "    # Here, \"0\" indicates CBCP\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 3] = 0\n",
    "    \n",
    "    avg_stats_array_rounded[2 * lambdas_index, 4] = np.round(avg_stats_array[2 * lambdas_index, 4], 2)\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 5] = np.round(avg_stats_array[2 * lambdas_index, 5], 2)\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 6] = np.round(avg_stats_array[2 * lambdas_index, 6], 2)\n",
    "    \n",
    "    avg_stats_array_rounded[2 * lambdas_index, 7] = avg_stats_array[2 * lambdas_index, 7]\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 8] = avg_stats_array[2 * lambdas_index, 8]\n",
    "\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 9] = round_to_sig_figs(avg_stats_array[2 * lambdas_index, 9], 3)\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 10] = round_to_sig_figs(avg_stats_array[2 * lambdas_index, 10], 3)\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 11] = round_to_sig_figs(avg_stats_array[2 * lambdas_index, 11], 3)\n",
    "    avg_stats_array_rounded[2 * lambdas_index, 12] = round_to_sig_figs(avg_stats_array[2 * lambdas_index, 12], 3)\n",
    "    \n",
    "    # Storing DBCP avg stats as array:\n",
    "    \n",
    "    dict_index = (lambda_E, lambda_R, lambda_I, 'DBCP')\n",
    "    \n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 0] = lambda_E\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 1] = lambda_R\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 2] = lambda_I\n",
    "    \n",
    "    # Here, \"1\" indicates DBCP\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 3] = 1\n",
    "    \n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 4] = np.round(avg_stats_array[2 * lambdas_index + 1, 4], 2)\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 5] = np.round(avg_stats_array[2 * lambdas_index + 1, 5], 2)\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 6] = np.round(avg_stats_array[2 * lambdas_index + 1, 6], 2)\n",
    "    \n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 7] = avg_stats_array[2 * lambdas_index + 1, 7]\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 8] = avg_stats_array[2 * lambdas_index + 1, 8]\n",
    "\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 9] = round_to_sig_figs(avg_stats_array[2 * lambdas_index + 1, 9], 3)\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 10] = round_to_sig_figs(avg_stats_array[2 * lambdas_index + 1, 10], 3)\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 11] = round_to_sig_figs(avg_stats_array[2 * lambdas_index + 1, 11], 3)\n",
    "    avg_stats_array_rounded[2 * lambdas_index + 1, 12] = round_to_sig_figs(avg_stats_array[2 * lambdas_index + 1, 12], 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2736ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.all(avg_stats_array_rounded == avg_stats_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ffe5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11323664",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "column_names += [\"lambda_E\", \"lambda_R\", \"lambda_I\"]\n",
    "column_names += [\"CBCP (=0) or DBCP (=1)\"]\n",
    "column_names += [\"% overall users using express lanes\", \\\n",
    "                 \"% eligible users using express lanes\", \\\n",
    "                 \"% ineligible users using express lanes\", \\\n",
    "                 \"Average travel time (express lanes)\", \\\n",
    "                 \"Average travel time (general purpose lanes)\", \\\n",
    "                 \"Total travel cost (eligible users)\", \\\n",
    "                 \"Total travel cost (ineligible users)\", \\\n",
    "                 \"Total toll revenue\", \\\n",
    "                 \"Total societal cost\"]\n",
    "\n",
    "df_avg_stats_to_save = pd.DataFrame(avg_stats_array_rounded, index=None, columns=column_names)\n",
    "\n",
    "df_avg_stats_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_to_save = \"../data/stats_compare___\" + str(num_el) + \"_el_groups/\"\n",
    "# directory_to_save = \"../data/stats_compare___\" + str(num_el) + \"_el_groups___before_20251001/\"\n",
    "\n",
    "# filename = \"opt_CBCP_params___\" + random_string + '.csv'\n",
    "\n",
    "filename = 'avg_stats___' + str(num_el) + '_el_groups.csv'\n",
    "\n",
    "df_avg_stats_to_save.to_csv(directory_to_save + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f380fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Stats array are: DBCP minus CBCP\n",
    "\n",
    "# stats_compare_percentage_array = np.zeros((lambdas_array.shape[0], 11))\n",
    "\n",
    "# for lambdas_index in range(lambdas_array.shape[0]):\n",
    "# # for lambdas_index in [12]:\n",
    "#     print()\n",
    "#     print(\"lambdas_index:\", lambdas_index)\n",
    "#     print(\"avg_stats_array[2 * lambdas_index, 4:12]:\\n\", avg_stats_array[2 * lambdas_index, 4:12])\n",
    "    \n",
    "#     stats_compare_percentage_array[lambdas_index, 0:3] = lambdas_array[lambdas_index, 0:3]\n",
    "    \n",
    "#     stats_compare_percentage_array[lambdas_index, 3:] = \\\n",
    "#         100 * (avg_stats_array[2 * lambdas_index + 1, 4:12] - avg_stats_array[2 * lambdas_index, 4:12]) \\\n",
    "#             / avg_stats_array[2 * lambdas_index, 4:12]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031eb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stats_compare_percentage_array\n",
    "# lambdas_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b036745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stats array are: DBCP minus CBCP\n",
    "\n",
    "## Percentage difference:\n",
    "\n",
    "# stats_compare_percentage_array = np.zeros((lambdas_array.shape[0], 12))\n",
    "\n",
    "# for lambdas_index in range(lambdas_array.shape[0]):\n",
    "#     print()\n",
    "#     print(\"lambdas_index:\", lambdas_index)\n",
    "#     print(\"avg_stats_array[2 * lambdas_index, 4:13]:\\n\", avg_stats_array[2 * lambdas_index, 4:13])\n",
    "    \n",
    "#     stats_compare_percentage_array[lambdas_index, 0:3] = lambdas_array[lambdas_index, 0:3]\n",
    "    \n",
    "#     stats_compare_percentage_array[lambdas_index, 3:] = \\\n",
    "#         100 * (avg_stats_array[2 * lambdas_index + 1, 4:13] - avg_stats_array[2 * lambdas_index, 4:13]) \\\n",
    "#             / np.abs(avg_stats_array[2 * lambdas_index, 4:13])\n",
    "\n",
    "\n",
    "stats_compare_absolute_percentage_array = np.zeros((lambdas_array.shape[0]*2, 13))\n",
    "\n",
    "for lambdas_index in range(lambdas_array.shape[0]):\n",
    "    print()\n",
    "    print(\"lambdas_index:\", lambdas_index)\n",
    "    print(\"avg_stats_array[2 * lambdas_index, 4:13]:\\n\", avg_stats_array[2 * lambdas_index, 4:13])\n",
    "    \n",
    "    stats_compare_absolute_percentage_array[2 * lambdas_index, 0:3] \\\n",
    "        = np.round(lambdas_array[lambdas_index, 0:3], decimals=2)\n",
    "    stats_compare_absolute_percentage_array[2 * lambdas_index, 3] = 0\n",
    "#     stats_compare_absolute_percentage_array[2 * lambdas_index, 4:13] = \\\n",
    "#         avg_stats_array[2 * lambdas_index + 1, 4:13] - avg_stats_array[2 * lambdas_index, 4:13]\n",
    "    stats_compare_absolute_percentage_array[2 * lambdas_index, 4:10] \\\n",
    "        = np.round(avg_stats_array[2 * lambdas_index + 1, 4:10] - avg_stats_array[2 * lambdas_index, 4:10], decimals=2)\n",
    "    stats_compare_absolute_percentage_array[2 * lambdas_index, 10:13] \\\n",
    "        = round_to_sig_figs_array_1D(avg_stats_array[2 * lambdas_index + 1, 10:13] - avg_stats_array[2 * lambdas_index, 10:13], 3)\n",
    "    \n",
    "    stats_compare_absolute_percentage_array[2 * lambdas_index + 1, 0:3] \\\n",
    "        = np.round(lambdas_array[lambdas_index, 0:3], decimals=2)\n",
    "    stats_compare_absolute_percentage_array[2 * lambdas_index + 1, 3] = 1\n",
    "    stats_compare_absolute_percentage_array[2 * lambdas_index + 1, 4:] \\\n",
    "        = np.round(100 * (avg_stats_array[2 * lambdas_index + 1, 4:13] - avg_stats_array[2 * lambdas_index, 4:13]) \\\n",
    "                 / np.abs(avg_stats_array[2 * lambdas_index, 4:13]), decimals=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6396cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "column_names += [\"lambda_E\", \"lambda_R\", \"lambda_I\"]\n",
    "column_names += [\"Absolute (=0) or Relative (=1) Difference\"]\n",
    "column_names += [\"Diff, % overall users using express lanes\", \\\n",
    "                 \"Diff, % eligible users using express lanes\", \\\n",
    "                 \"Diff, % ineligible users using express lanes\", \\\n",
    "                 \"Diff, Average travel time (express lanes)\", \\\n",
    "                 \"Diff, Average travel time (general purpose lanes)\", \\\n",
    "                 \"Diff, Total travel cost (eligible users)\", \\\n",
    "                 \"Diff, Total travel cost (ineligible users)\", \\\n",
    "                 \"Diff, Total toll revenue\", \\\n",
    "                 \"Diff, Total societal cost\"]\n",
    "\n",
    "df_stats_compare_to_save = pd.DataFrame(stats_compare_absolute_percentage_array, index=None, columns=column_names)\n",
    "\n",
    "df_stats_compare_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = []\n",
    "# column_names += [\"lambda_E\", \"lambda_R\", \"lambda_I\"]\n",
    "# column_names += [\"% overall users using express lanes\", \\\n",
    "#                  \"% eligible users using express lanes\", \\\n",
    "#                  \"% ineligible users using express lanes\", \\\n",
    "#                  \"Average travel time (express lanes)\", \\\n",
    "#                  \"Average travel time (general purpose lanes)\", \\\n",
    "#                  \"Total travel cost (eligible users)\", \\\n",
    "#                  \"Total travel cost (ineligible users)\", \\\n",
    "#                  \"Total toll revenue\"]\n",
    "\n",
    "# df_stats_compare_to_save = pd.DataFrame(stats_compare_percentage_array, index=None, columns=column_names)\n",
    "\n",
    "# df_stats_compare_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18353bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08479069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_to_save = \"../data/stats_compare/\"\n",
    "# filename = \"opt_CBCP_params___\" + random_string + '.csv'\n",
    "\n",
    "filename = 'compare_stats___' + str(num_el) + '_el_groups.csv'\n",
    "\n",
    "df_stats_compare_to_save.to_csv(directory_to_save + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ebdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f97b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965f772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe3818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b39d8cdb",
   "metadata": {},
   "source": [
    "## <font color='red'>STOP. End of Code to run.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211d306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135abc98",
   "metadata": {},
   "source": [
    "# End of code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d4579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0002eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(np.array([1, 2, 3]) * np.array([5, 3, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_travel_time_CBCP_dict[(1.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a259336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau_CBCP_dict = {}\n",
    "# tau_time_averaged_CBCP_dict = {}\n",
    "# B_CBCP_dict = {}\n",
    "# percent_express_lane_use_CBCP_dict = {}\n",
    "# avg_travel_time_CBCP_dict = {}\n",
    "# total_costs_CBCP_dict = {}\n",
    "\n",
    "# tau_DBCP_dict = {}\n",
    "# tau_time_averaged_DBCP_dict = {}\n",
    "# alpha_DBCP_dict = {}\n",
    "# percent_express_lane_use_DBCP_dict = {}\n",
    "# avg_travel_time_DBCP_dict = {}\n",
    "# total_costs_DBCP_dict = {}\n",
    "\n",
    "\n",
    "# directory_CBCP = '../data/opt_CBCP_values___' + str(num_el) + '_el_groups/'\n",
    "# directory_DBCP = '../data/opt_DBCP_values___' + str(num_el) + '_el_groups/'\n",
    "\n",
    "# lambdas_index = 4\n",
    "# lambda_E, lambda_R, lambda_I = lambdas_array[lambdas_index]\n",
    "# lambdas = (lambda_E, lambda_R, lambda_I)\n",
    "# filename_segment = str(int(lambda_E)) + '_' + str(int(lambda_R)) + '_' + str(int(lambda_I))\n",
    "\n",
    "# # Inputting CBCP data:\n",
    "\n",
    "# df_CBCP = pd.read_csv(directory_CBCP + filename_segment + '___tau_B_stats_CBCP.csv')\n",
    "# tau_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 1:6]\n",
    "# tau_time_averaged_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 6]\n",
    "# B_CBCP_dict[lambdas] = df_CBCP.to_numpy()[0, 7]\n",
    "# percent_express_lane_use_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 8:11]\n",
    "# avg_travel_time_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 11:12]\n",
    "# total_costs_CBCP_dict[lambdas] = df_CBCP.to_numpy()[:, 12:]\n",
    "\n",
    "# # Inputting DBCP data:\n",
    "\n",
    "# df_DBCP = pd.read_csv(directory_DBCP + filename_segment + '___tau_alpha_stats_DBCP.csv')\n",
    "# tau_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 1:6]\n",
    "# tau_time_averaged_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 6]\n",
    "# alpha_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 7:12]\n",
    "# percent_express_lane_use_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 12:15]\n",
    "# avg_travel_time_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 15:16]\n",
    "# total_costs_DBCP_dict[lambdas] = df_DBCP.to_numpy()[:, 16:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad08e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969e6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe7d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5bd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "511ac2c6",
   "metadata": {},
   "source": [
    "## Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377dbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67fc9ddd",
   "metadata": {},
   "source": [
    "## <font color='red'>Colored Font Titles</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21358e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dc84df4",
   "metadata": {},
   "source": [
    "# Scratch Work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cp.Variable(2)\n",
    "y = cp.Variable(2)\n",
    "v_fixed = np.array([0, 1])\n",
    "objective = cp.Minimize(cp.sum_squares(x - y) + cp.sum_squares(x - v_fixed))\n",
    "constraints = []\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "# The optimal value for x is stored in `x.value`.\n",
    "print(\"x.value:\", x.value)\n",
    "print(\"y.value:\", y.value)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cc25f",
   "metadata": {},
   "source": [
    "## Linear Approximation for Latency Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables:\n",
    "v = cp.Variable(1)\n",
    "            \n",
    "# Objective:\n",
    "func = v - 1 + cp.square(cp.maximum(v-1, 0))\n",
    "objective = cp.Minimize(func)\n",
    "\n",
    "# Constraints:\n",
    "constraints = [-3.0 <= v, v <= 3.0]\n",
    "\n",
    "# Solve problem:\n",
    "prob = cp.Problem(objective, constraints)\n",
    "result = prob.solve()\n",
    "\n",
    "# Print solution:\n",
    "print(\"v.value:\", v.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b7479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
